{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48850c3-0eac-42ce-b9bd-46fc1304bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "df = pd.read_csv(\"raw_data.csv\")\n",
    "pkup_df = df[df['site_name'] == 'GRSM-PKnob Upper Plot']\n",
    "pkup_plants = pkup_df[pkup_df[\"kingdom\"] == \"Plantae\"]\n",
    "\n",
    "def filter_species_with_min_days(df, min_days=10):\n",
    "    \"\"\"\n",
    "    Filters species (and phenophases) that have at least `min_days` unique first_yes_days \n",
    "    corresponding to the first observed day per year.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing columns:\n",
    "                           'species_id', 'phenophase_id', 'first_yes_year', 'first_yes_day'.\n",
    "        min_days (int): Minimum number of unique first_yes_days required to keep a species.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing only species/phenophases with at least `min_days` valid entries.\n",
    "    \"\"\"\n",
    "    # Ensure we're only working with the first observed day per year\n",
    "    first_observed_df = df.sort_values(['species_id', 'phenophase_id', 'first_yes_year', 'first_yes_day']).drop_duplicates(\n",
    "        subset=['species_id', 'phenophase_id', 'first_yes_year'], keep='first'\n",
    "    )\n",
    "\n",
    "    # Count the number of unique first_yes_days for each species/phenophase\n",
    "    valid_counts = first_observed_df.groupby(['species_id', 'phenophase_id'])['first_yes_day'].nunique()\n",
    "\n",
    "    # Filter for species/phenophases with at least min_days unique first_yes_days\n",
    "    valid_species = valid_counts[valid_counts >= min_days].index\n",
    "\n",
    "    # Return the filtered DataFrame\n",
    "    filtered_df = first_observed_df[\n",
    "        first_observed_df.set_index(['species_id', 'phenophase_id']).index.isin(valid_species)\n",
    "    ]\n",
    "    \n",
    "    return filtered_df.reset_index(drop=True)\n",
    "\n",
    "# Usage\n",
    "filtered_pkup_plants = filter_species_with_min_days(pkup_plants, min_days=10)\n",
    "filtered_pkup_plants\n",
    "print(len(filtered_pkup_plants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadb6f1-c9f4-4388-b7af-154e2654bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_species_phenophase_pairs(data):\n",
    "    \"\"\"\n",
    "    Finds all species pairs with all possible phenophase pairs based on the first_yes_day.\n",
    "    Ensures valid matches only include years where both species have observations.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): The filtered dataset containing columns for species_id, phenophase_id,\n",
    "                          first_yes_day, and first_yes_year.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Contains columns for species pairs, phenophase pairs, years, and first_yes_days.\n",
    "    \"\"\"\n",
    "    # Filter required columns\n",
    "    filtered_data = data[['species_id', 'phenophase_id', 'first_yes_day', 'first_yes_year']].dropna()\n",
    "\n",
    "    # Ensure only the first `first_yes_day` for each species-phenophase-year combination is considered\n",
    "    filtered_data = filtered_data.sort_values('first_yes_day').drop_duplicates(\n",
    "        subset=['species_id', 'phenophase_id', 'first_yes_year']\n",
    "    )\n",
    "\n",
    "    # Initialize a list to store pairs\n",
    "    pairs = []\n",
    "\n",
    "    # Create all possible pairs of species and phenophases\n",
    "    grouped_data = filtered_data.groupby(['species_id', 'phenophase_id'])\n",
    "    for (species_a, phenophase_a), group_a in grouped_data:\n",
    "        for (species_b, phenophase_b), group_b in grouped_data:\n",
    "            # Avoid duplicate pairs (A-B and B-A are the same) and self-pairs unless explicitly allowed\n",
    "            if (species_a, phenophase_a) < (species_b, phenophase_b):\n",
    "                # Find common years between the two groups\n",
    "                common_years = set(group_a['first_yes_year']).intersection(group_b['first_yes_year'])\n",
    "\n",
    "                # If there are common years, add the pair details\n",
    "                for year in common_years:\n",
    "                    day_a = group_a[group_a['first_yes_year'] == year]['first_yes_day'].values[0]\n",
    "                    day_b = group_b[group_b['first_yes_year'] == year]['first_yes_day'].values[0]\n",
    "                    pairs.append({\n",
    "                        'species_a': species_a,\n",
    "                        'phenophase_a': phenophase_a,\n",
    "                        'species_b': species_b,\n",
    "                        'phenophase_b': phenophase_b,\n",
    "                        'year': year,\n",
    "                        'first_yes_day_a': day_a,\n",
    "                        'first_yes_day_b': day_b\n",
    "                    })\n",
    "\n",
    "    # Return a DataFrame of the pairs\n",
    "    return pd.DataFrame(pairs)\n",
    "filtered = find_species_phenophase(filtered_pkup_plants)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def7aec5-712d-430e-be3c-3e324570b25f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_valid_combos\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Apply the function to get valid combinations\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m combos \u001b[38;5;241m=\u001b[39m get_valid_combinations(pkup_plants)\n\u001b[1;32m     63\u001b[0m combos\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mget_valid_combinations\u001b[0;34m(original_df)\u001b[0m\n\u001b[1;32m     46\u001b[0m             valid_combinations\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     47\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies_one\u001b[39m\u001b[38;5;124m'\u001b[39m: species_one,\n\u001b[1;32m     48\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies_two\u001b[39m\u001b[38;5;124m'\u001b[39m: species_two,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_yes_days_species_two\u001b[39m\u001b[38;5;124m'\u001b[39m: plant_two_days\n\u001b[1;32m     52\u001b[0m             })\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame and remove duplicates\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m final_valid_combos \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(valid_combinations)\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m     56\u001b[0m final_valid_combos \u001b[38;5;241m=\u001b[39m final_valid_combos\u001b[38;5;241m.\u001b[39msort_values(\n\u001b[1;32m     57\u001b[0m     by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies_one\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies_two\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_valid_combos\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:6818\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6815\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6816\u001b[0m ignore_index \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 6818\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduplicated(subset, keep\u001b[38;5;241m=\u001b[39mkeep)]\n\u001b[1;32m   6819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[1;32m   6820\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:6958\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6957\u001b[0m     vals \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[0;32m-> 6958\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(f, vals)))\n\u001b[1;32m   6960\u001b[0m     ids \u001b[38;5;241m=\u001b[39m get_group_index(labels, \u001b[38;5;28mtuple\u001b[39m(shape), sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   6961\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:6926\u001b[0m, in \u001b[0;36mDataFrame.duplicated.<locals>.f\u001b[0;34m(vals)\u001b[0m\n\u001b[1;32m   6925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(vals) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m-> 6926\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m algorithms\u001b[38;5;241m.\u001b[39mfactorize(vals, size_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m   6927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:795\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[1;32m    793\u001b[0m             values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[0;32m--> 795\u001b[0m     codes, uniques \u001b[38;5;241m=\u001b[39m factorize_array(\n\u001b[1;32m    796\u001b[0m         values,\n\u001b[1;32m    797\u001b[0m         use_na_sentinel\u001b[38;5;241m=\u001b[39muse_na_sentinel,\n\u001b[1;32m    798\u001b[0m         size_hint\u001b[38;5;241m=\u001b[39msize_hint,\n\u001b[1;32m    799\u001b[0m     )\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    802\u001b[0m     uniques, codes \u001b[38;5;241m=\u001b[39m safe_sort(\n\u001b[1;32m    803\u001b[0m         uniques,\n\u001b[1;32m    804\u001b[0m         codes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    808\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:595\u001b[0m, in \u001b[0;36mfactorize_array\u001b[0;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    592\u001b[0m hash_klass, values \u001b[38;5;241m=\u001b[39m _get_hashtable_algo(values)\n\u001b[1;32m    594\u001b[0m table \u001b[38;5;241m=\u001b[39m hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[0;32m--> 595\u001b[0m uniques, codes \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfactorize(\n\u001b[1;32m    596\u001b[0m     values,\n\u001b[1;32m    597\u001b[0m     na_sentinel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    598\u001b[0m     na_value\u001b[38;5;241m=\u001b[39mna_value,\n\u001b[1;32m    599\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m    600\u001b[0m     ignore_na\u001b[38;5;241m=\u001b[39muse_na_sentinel,\n\u001b[1;32m    601\u001b[0m )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[1;32m    604\u001b[0m uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7281\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7195\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "def get_valid_combinations(original_df):\n",
    "    # Initialize a list to store valid combinations\n",
    "    valid_combinations = []\n",
    "    \n",
    "    # Generate all possible combinations of species\n",
    "    all_species_combinations = original_df.merge(\n",
    "        original_df, how='cross', suffixes=('_one', '_two')\n",
    "    )\n",
    "    \n",
    "    # Iterate through each combination\n",
    "    for _, row in all_species_combinations.iterrows():\n",
    "        # Ensure they are not the same species\n",
    "        if row['species_id_one'] != row['species_id_two']:\n",
    "            # Get the years and first_yes_days for species one\n",
    "            plant_one_data = original_df[\n",
    "                (original_df['species_id'] == row['species_id_one'])\n",
    "            ][['first_yes_year', 'first_yes_day']].drop_duplicates().sort_values('first_yes_year')\n",
    "            \n",
    "            # Get the years and first_yes_days for species two\n",
    "            plant_two_data = original_df[\n",
    "                (original_df['species_id'] == row['species_id_two'])\n",
    "            ][['first_yes_year', 'first_yes_day']].drop_duplicates().sort_values('first_yes_year')\n",
    "            \n",
    "            # Extract years and days as sets\n",
    "            plant_one_years = set(plant_one_data['first_yes_year'])\n",
    "            plant_two_years = set(plant_two_data['first_yes_year'])\n",
    "            \n",
    "            # Find the intersection of years\n",
    "            matching_years = plant_one_years.intersection(plant_two_years)\n",
    "            \n",
    "            # Check if there are at least 10 matching years\n",
    "            if len(matching_years) >= 10:\n",
    "                matching_years = sorted(matching_years)\n",
    "                \n",
    "                # Extract corresponding first_yes_days for the matching years\n",
    "                plant_one_days = [\n",
    "                    plant_one_data[plant_one_data['first_yes_year'] == year]['first_yes_day'].iloc[0]\n",
    "                    for year in matching_years\n",
    "                ]\n",
    "                plant_two_days = [\n",
    "                    plant_two_data[plant_two_data['first_yes_year'] == year]['first_yes_day'].iloc[0]\n",
    "                    for year in matching_years\n",
    "                ]\n",
    "                \n",
    "                species_one, species_two = sorted([row['species_id_one'], row['species_id_two']])\n",
    "                valid_combinations.append({\n",
    "                    'species_one': species_one,\n",
    "                    'species_two': species_two,\n",
    "                    'matching_years': tuple(matching_years),\n",
    "                    'first_yes_days_species_one': plant_one_days,\n",
    "                    'first_yes_days_species_two': plant_two_days\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and remove duplicates\n",
    "    final_valid_combos = pd.DataFrame(valid_combinations).drop_duplicates()\n",
    "    final_valid_combos = final_valid_combos.sort_values(\n",
    "        by=['species_one', 'species_two']\n",
    "    ).reset_index(drop=True)\n",
    "    return final_valid_combos\n",
    "\n",
    "# Apply the function to get valid combinations\n",
    "combos = get_valid_combinations(pkup_plants)\n",
    "combos\n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3e3d8-d421-429b-b647-10b02270dd42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
